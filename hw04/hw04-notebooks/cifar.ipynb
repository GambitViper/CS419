{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow\n",
    "import numpy\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "# Plot ad hoc CIFAR10 instances\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "# init random seed for reproductability\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting GPU memory usage\n",
    "\n",
    "The code here should be added to any work you do on Volta.  If you don't, then your code will monopolize all available memory on each of the 4 GPUs on the machine, preventing others from working on it.  If you do **that**, you will be frowned upon.\n",
    "\n",
    "The code in the next cell has the effect that:\n",
    "1. Memory use will start off with some small fraction of the memory on each GPU.\n",
    "1. It will grow if necessary (since `allow_growth` is set to `True`).\n",
    "1. It will max out at 5% of overall memory.  Given the GPUs we have, this gives you (4 x 808 MB), which should be sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Limit TensorFlow GPU use.\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "K.tensorflow_backend.set_session(tensorflow.Session(config=config))\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "Loading the training and testing data is from the `keras` with the built-in dataset tools\n",
    "1. Normalizing the data inputs takes the pixel array information and divides by max possible value\n",
    "1. This fixes the problem of the neural network model from finding patterns based on magnitude of the values rather than connected patters of similar color in a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# normalizing inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting classification training and testing data to a one-hot vector form\n",
    "`to_categorical` converts this into a matrix with as many columns as there are classes. The number of rows stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the outputs\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "num_classes = Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and evaluating our model\n",
    "\n",
    "The initial model initial configuration (6 convolutions of size 3x3 with neurons from 32, 64, and 128; 3 max pooling layers using size 2x2; 3 dropout layers of 0.2; 2 hidden layers of size 1024, 512 using maxnorm kernel; 3 dropout layers into the hidden layers; categorical-cross entropy with sgd as an optimizer)\n",
    "\n",
    "The stocastic gradient descent omptimizing function fixes the training along the learning rate of the network\n",
    "\n",
    "The rmsprop optimizing function is a different but similar approach to optimizing network training within a learning rate of the network.\n",
    "\n",
    "The loss function is categorical cross entropy (and so we want to measure the associated binary accuracy value based on the category chosen by our network). The model is trained using a batch size equal to 25 epochs.\n",
    "\n",
    "1. Categorical cross entropy is a loss function for evaluating the updates for the weights in the network. Specifically for when a classification problem has a categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 64, 16, 16)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,168,362\n",
      "Trainable params: 2,168,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "# opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traceback for program concurrent epochs\n",
    "\n",
    "Setting the random seed ensures that the randomness applied to our network is a consistent randomness so we can monitor how changes to the model effect the accuracy of the classifications.\n",
    "\n",
    "Fitting the model to the training data and testing against the testing data for each epoch with a variable batch size per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 1.8399 - acc: 0.3217 - val_loss: 1.4470 - val_acc: 0.4701\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 1.4199 - acc: 0.4834 - val_loss: 1.2330 - val_acc: 0.5581\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 1.2338 - acc: 0.5551 - val_loss: 1.1174 - val_acc: 0.6016\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 1.1017 - acc: 0.6072 - val_loss: 0.9685 - val_acc: 0.6539\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 1.0044 - acc: 0.6426 - val_loss: 0.8908 - val_acc: 0.6838\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.9298 - acc: 0.6720 - val_loss: 0.8589 - val_acc: 0.6941\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.8651 - acc: 0.6935 - val_loss: 0.8065 - val_acc: 0.7154\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.8217 - acc: 0.7091 - val_loss: 0.7745 - val_acc: 0.7261\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.7800 - acc: 0.7248 - val_loss: 0.7646 - val_acc: 0.7305\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.7428 - acc: 0.7358 - val_loss: 0.7325 - val_acc: 0.7454\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.7136 - acc: 0.7461 - val_loss: 0.7280 - val_acc: 0.7451\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.6909 - acc: 0.7569 - val_loss: 0.7009 - val_acc: 0.7527\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.6603 - acc: 0.7662 - val_loss: 0.6942 - val_acc: 0.7581\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.6410 - acc: 0.7735 - val_loss: 0.6856 - val_acc: 0.7593\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.6230 - acc: 0.7804 - val_loss: 0.6702 - val_acc: 0.7664\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.6022 - acc: 0.7863 - val_loss: 0.6831 - val_acc: 0.7630\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.5838 - acc: 0.7937 - val_loss: 0.6577 - val_acc: 0.7706\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 0.5690 - acc: 0.7989 - val_loss: 0.6492 - val_acc: 0.7748\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.5537 - acc: 0.8031 - val_loss: 0.6620 - val_acc: 0.7718\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.5375 - acc: 0.8098 - val_loss: 0.6463 - val_acc: 0.7790\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.5283 - acc: 0.8108 - val_loss: 0.6446 - val_acc: 0.7751\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.5131 - acc: 0.8182 - val_loss: 0.6494 - val_acc: 0.7802\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.5023 - acc: 0.8221 - val_loss: 0.6444 - val_acc: 0.7802\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.4908 - acc: 0.8251 - val_loss: 0.6469 - val_acc: 0.7803\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.4772 - acc: 0.8307 - val_loss: 0.6378 - val_acc: 0.7845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fe0745dd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of models on training data\n",
    "Training data accuracy on initial configuration (6 convolutions of size 3x3 with neurons from 32, 64, and 128; 3 max pooling layers using size 2x2; 3 dropout layers of 0.2; 2 hidden layers of size 1024, 512 using maxnorm kernel; 3 dropout layers into the hidden layers; categorical-cross entropy with sgd as an optimizer) is 92.60%\n",
    "\n",
    "Training data accuracy on a modified configuration of changing 2 of the convolutions to be of size 5x5 without any other changes makes the categorical classification doesn't make much of an impact on the training accuracy at 92.57%\n",
    "\n",
    "Training data accuracy on a different model with RMSprop optimizer (4 convolutions of size 3x3 with neurons from 32, and 64; 2 max pooling layers using size 2x2; 2 dropout layers of 0.25; 2 hidden layers of size 512 and num_classes; 1 dropout layer in the hidden layers of 0.5; categorical-cross entropy loss function and RMSprop optimizer and a epoch size of 100 and batch size of 32) is 84.80%\n",
    "\n",
    "Training data accuracy on a similar model with SGD optimizer (4 convolutions of size 3x3 with neurons from 32, and 64; 2 max pooling layers using size 2x2; 2 dropout layers of 0.25; 2 hidden layers of size 512 and num_classes; 1 dropout layer in the hidden layers of 0.5; categorical-cross entropy loss function and sgd optimizer and a epoch size of 50 and batch size of 32) is 99.85%\n",
    "\n",
    "Training data accuracy with same model above with a smaller epoch size to avoid the overfitting found in the last iteration is 92.49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.49%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on training data\n",
    "scores = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of models on testing data\n",
    "Testing data accuracy on initial configuration (6 convolutions of size 3x3 with neurons from 32, 64, and 128; 3 max pooling layers using size 2x; 3 dropout layers of 0.2; 2 hidden layers of size 1024, 512 using maxnorm kernel; 3 dropout layers into the hidden layers; categorical-cross entropy with sgd as an optimizer) is 80.17%\n",
    "\n",
    "Testing data accuracy on a modified configuration of changing 2 of the convolutions to be of size 5x5 without any other changes makes the categorical classification slightly worse at 79.77%\n",
    "\n",
    "Testing data accuracy on a different model with RMSprop optimizer (4 convolutions of size 3x3 with neurons from 32, and 64; 2 max pooling layers using size 2x2; 2 dropout layers of 0.25; 2 hidden layers of size 512 and num_classes; 1 dropout layer in the hidden layers of 0.5; categorical-cross entropy loss function and RMSprop optimizer and a epoch size of 100 and batch size of 32) is 78.30%\n",
    "\n",
    "Testing data accuracy on a similar model with SGD optimizer (4 convolutions of size 3x3 with neurons from 32, and 64; 2 max pooling layers using size 2x2; 2 dropout layers of 0.25; 2 hidden layers of size 512 and num_classes; 1 dropout layer in the hidden layers of 0.5; categorical-cross entropy loss function and sgd optimizer and a epoch size of 50 and batch size of 32) is 80.25%\n",
    "\n",
    "Testing data accuracy with same model above with a smaller epoch size to avoid the overfitting found in the last iteration is 78.45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.45%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on training data\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
